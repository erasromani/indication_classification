{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gpfs/data/geraslab/ekr6072/projects/study_indication/data')\n",
    "data_path = data_dir / 'dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'rb') as f:\n",
    "  dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "  output = {}\n",
    "  for name, subset in dataset.items():\n",
    "    clean_subset = []\n",
    "    for data in subset:\n",
    "      label = data['label']\n",
    "      if label not in ['exclude', 'unknown']:\n",
    "        clean_subset.append(data)\n",
    "    output[name] = clean_subset\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = clean_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2id = {\n",
    "  '(high-risk) screening': 0,\n",
    "  'extent of disease / pre-operative planning': 1,\n",
    "  'additional workup': 2,\n",
    "  '6-month follow-up / surveillance': 3,\n",
    "  'treatment monitoring': 4,\n",
    "  'exclude': 5,\n",
    "  'unknown': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [data['text']['longText'] for data in dataset['train']]\n",
    "train_labels = [category2id[data['label']] for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [data['text']['longText'] for data in dataset['val']]\n",
    "val_labels = [category2id[data['label']] for data in dataset['val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=None)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for sequence in train_encodings['input_ids']:\n",
    "  pad = np.where(np.array(sequence) == 1)[0]\n",
    "  if len(pad) == 0:\n",
    "    length = len(sequence)\n",
    "  else: \n",
    "    length = pad.min()\n",
    "  lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(lengths, bins=10);\n",
    "ax.set_xlabel('token count per example'); # add logits from all the classifiers and backprop through sum of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IndicationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IndicationDataset(train_encodings, train_labels)\n",
    "val_dataset = IndicationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 1500])\n",
      "attention_mask torch.Size([2, 1500])\n",
      "labels torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for key, val in batch.items():\n",
    "  print(key, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: value.to(device) for key, value in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-94a1b5ff36f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 )\n\u001b[1;32m   1298\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         )\n\u001b[1;32m   1222\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1158\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         attn_scores = self._sliding_chunks_query_key_matmul(\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mquery_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         )\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mdiagonal_chunked_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bcxd,bcyd->bcxy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# convert diagonals into columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee3b5efc781680bf777f6c0f6ded34256a070c94fe240b983fb244a284c14fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ds_1012': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
