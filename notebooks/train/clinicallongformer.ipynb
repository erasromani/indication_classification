{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gpfs/data/geraslab/ekr6072/projects/study_indication/data')\n",
    "data_path = data_dir / 'dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'rb') as f:\n",
    "  dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "  output = {}\n",
    "  for name, subset in dataset.items():\n",
    "    clean_subset = []\n",
    "    for data in subset:\n",
    "      label = data['label']\n",
    "      if label not in ['exclude', 'unknown']:\n",
    "        clean_subset.append(data)\n",
    "    output[name] = clean_subset\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = clean_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2id = {\n",
    "  '(high-risk) screening': 0,\n",
    "  'extent of disease / pre-operative planning': 1,\n",
    "  'additional workup': 2,\n",
    "  '6-month follow-up / surveillance': 3,\n",
    "  'treatment monitoring': 4,\n",
    "  'exclude': 5,\n",
    "  'unknown': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [data['text']['longText'] for data in dataset['train']]\n",
    "train_labels = [category2id[data['label']] for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [data['text']['longText'] for data in dataset['val']]\n",
    "val_labels = [category2id[data['label']] for data in dataset['val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=1500)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for sequence in train_encodings['input_ids']:\n",
    "  pad = np.where(np.array(sequence) == 1)[0]\n",
    "  if len(pad) == 0:\n",
    "    length = len(sequence)\n",
    "  else: \n",
    "    length = pad.min()\n",
    "  lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(lengths, bins=10);\n",
    "ax.set_xlabel('token count per example'); # add logits from all the classifiers and backprop through sum of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IndicationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IndicationDataset(train_encodings, train_labels)\n",
    "val_dataset = IndicationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 1500])\n",
      "attention_mask torch.Size([2, 1500])\n",
      "labels torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for key, val in batch.items():\n",
    "  print(key, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: value.to(device) for key, value in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-94a1b5ff36f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 )\n\u001b[1;32m   1298\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         )\n\u001b[1;32m   1222\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1158\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         attn_scores = self._sliding_chunks_query_key_matmul(\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mquery_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         )\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mdiagonal_chunked_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bcxd,bcyd->bcxy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# convert diagonals into columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee3b5efc781680bf777f6c0f6ded34256a070c94fe240b983fb244a284c14fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ds_1012': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
