{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gpfs/data/geraslab/ekr6072/projects/study_indication/data')\n",
    "data_path = data_dir / 'dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'rb') as f:\n",
    "  dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "  output = {}\n",
    "  for name, subset in dataset.items():\n",
    "    clean_subset = []\n",
    "    for data in subset:\n",
    "      label = data['label']\n",
    "      if label not in ['exclude', 'unknown']:\n",
    "        clean_subset.append(data)\n",
    "    output[name] = clean_subset\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = clean_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2id = {\n",
    "  '(high-risk) screening': 0,\n",
    "  'extent of disease / pre-operative planning': 1,\n",
    "  'additional workup': 2,\n",
    "  '6-month follow-up / surveillance': 3,\n",
    "  'treatment monitoring': 4,\n",
    "  'exclude': 5,\n",
    "  'unknown': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [data['text']['longText'] for data in dataset['train']]\n",
    "train_labels = [category2id[data['label']] for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [data['text']['longText'] for data in dataset['val']]\n",
    "val_labels = [category2id[data['label']] for data in dataset['val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for sequence in train_encodings['input_ids']:\n",
    "  pad = np.where(np.array(sequence) == 1)[0]\n",
    "  if len(pad) == 0:\n",
    "    length = len(sequence)\n",
    "  else: \n",
    "    length = pad.min()\n",
    "  lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(lengths, bins=10);\n",
    "ax.set_xlabel('token count per example'); # add logits from all the classifiers and backprop through sum of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IndicationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IndicationDataset(train_encodings, train_labels)\n",
    "val_dataset = IndicationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"yikuan8/Clinical-Longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {key: value.to(device) for key, value in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-94a1b5ff36f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1712\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         )\n\u001b[1;32m   1716\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m                 )\n\u001b[1;32m   1298\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         )\n\u001b[1;32m   1222\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         )\n\u001b[1;32m   1158\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         attn_scores = self._sliding_chunks_query_key_matmul(\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mquery_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         )\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[0;31m# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;31m# bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0mdiagonal_chunked_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bcxd,bcyd->bcxy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# convert diagonals into columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data/geraslab/ekr6072/miniconda3/envs/ds_1012/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.69 GiB (GPU 0; 22.38 GiB total capacity; 19.57 GiB already allocated; 1.64 GiB free; 20.10 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee3b5efc781680bf777f6c0f6ded34256a070c94fe240b983fb244a284c14fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ds_1012': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
