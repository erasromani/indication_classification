{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import os\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/gpfs/data/geraslab/ekr6072/projects/study_indication/data')\n",
    "data_path = data_dir / 'dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ClinicalBERT(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "      super(ClinicalBERT, self).__init__()\n",
    "      self.bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "      self.linear = nn.Linear(768, num_classes)\n",
    "      self.loss_func = nn.CrossEntropyLoss()\n",
    "  \n",
    "  def forward(self, **kwargs):\n",
    "    x = self.bert(input_ids=kwargs['input_ids'], attention_mask=kwargs['attention_mask'])\n",
    "    logits = self.linear(x['pooler_output'])\n",
    "    label = kwargs['labels']\n",
    "    loss = self.loss_func(logits, label)\n",
    "    return {\n",
    "      \"loss\": loss,\n",
    "      \"logits\": logits\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path, 'rb') as f:\n",
    "  dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "  output = {}\n",
    "  for name, subset in dataset.items():\n",
    "    clean_subset = []\n",
    "    for data in subset:\n",
    "      label = data['label']\n",
    "      if label not in ['exclude', 'unknown']:\n",
    "        clean_subset.append(data)\n",
    "    output[name] = clean_subset\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = clean_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category2id = {\n",
    "  '(high-risk) screening': 0,\n",
    "  'extent of disease / pre-operative planning': 1,\n",
    "  'additional workup': 2,\n",
    "  '6-month follow-up / surveillance': 3,\n",
    "  'treatment monitoring': 4,\n",
    "  'exclude': 5,\n",
    "  'unknown': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [data['text']['longText'] for data in dataset['train']]\n",
    "train_labels = [category2id[data['label']] for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_texts = [data['text']['longText'] for data in dataset['val']]\n",
    "val_labels = [category2id[data['label']] for data in dataset['val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for sequence in train_encodings['input_ids']:\n",
    "  pad = np.where(np.array(sequence) == 0)[0]\n",
    "  if len(pad) == 0:\n",
    "    length = len(sequence)\n",
    "  else: \n",
    "    length = pad.min()\n",
    "  lengths.append(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(lengths, bins=10);\n",
    "ax.set_xlabel('token count per example'); # add logits from all the classifiers and backprop through sum of the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IndicationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IndicationDataset(train_encodings, train_labels)\n",
    "val_dataset = IndicationDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_iter(num_epochs, dataloader):\n",
    "    steps_per_epoch = len(dataloader)\n",
    "    for epoch in range(num_epochs):\n",
    "      for step in range(steps_per_epoch):\n",
    "        yield epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_loop(model, dataloader, device):\n",
    "    \"\"\"Run validation phase.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Keeping track of metrics\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0.0\n",
    "    total_count = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Only count non-padding tokens\n",
    "        # (Same idea as ignore_index=PAD_IDX above)\n",
    "        preds = outputs['logits'].argmax(-1)\n",
    "        labels = batch['labels']\n",
    "        correct_preds = (labels == preds).sum()\n",
    "        all_labels.append(labels)\n",
    "        all_preds.append(preds)\n",
    "\n",
    "        # Keeping track of metrics\n",
    "        total_loss += loss.item()\n",
    "        total_correct += correct_preds.item()\n",
    "        total_count += preds.shape[0]\n",
    "    all_labels = torch.cat(all_labels).cpu()\n",
    "    all_preds = torch.cat(all_preds).cpu()\n",
    "    return {\n",
    "        \"loss\": total_loss / total_count,\n",
    "        \"accuracy\": total_correct / total_count,\n",
    "        \"f1_score\": f1_score(all_labels, all_preds, average='macro')\n",
    "    }\n",
    "\n",
    "def train_step(optimizer, model, batch):\n",
    "    \"\"\"Run a single train step.\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs[\"loss\"]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step: int, warmup_steps: int, total_steps: int, decay_type='linear'):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    if decay_type is None:\n",
    "        return 1.0\n",
    "    elif decay_type == 'linear':\n",
    "        w = - 1 / (total_steps - warmup_steps)\n",
    "        return (current_step - warmup_steps) * w + 1.0\n",
    "    elif decay_type == 'cosine':\n",
    "        w = np.pi / (total_steps - warmup_steps)\n",
    "        return 0.5 * np.cos(w * (current_step - warmup_steps)) + 0.5\n",
    "    else:\n",
    "        raise ValueError('invalid decay_type {} entered'.format(decay_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5/360, val acc: 0.344, val f1: 0.142\n",
      "Step: 10/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 15/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 20/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 25/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 30/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 35/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 40/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 45/360, val acc: 0.656, val f1: 0.312\n",
      "Step: 50/360, val acc: 0.456, val f1: 0.125\n",
      "Step: 55/360, val acc: 0.678, val f1: 0.320\n",
      "Step: 60/360, val acc: 0.633, val f1: 0.303\n",
      "Step: 65/360, val acc: 0.689, val f1: 0.321\n",
      "Step: 70/360, val acc: 0.689, val f1: 0.317\n",
      "Step: 75/360, val acc: 0.711, val f1: 0.330\n",
      "Step: 80/360, val acc: 0.600, val f1: 0.412\n",
      "Step: 85/360, val acc: 0.689, val f1: 0.401\n",
      "Step: 90/360, val acc: 0.733, val f1: 0.465\n",
      "Step: 95/360, val acc: 0.656, val f1: 0.490\n",
      "Step: 100/360, val acc: 0.778, val f1: 0.516\n",
      "Step: 105/360, val acc: 0.556, val f1: 0.311\n",
      "Step: 110/360, val acc: 0.689, val f1: 0.506\n",
      "Step: 115/360, val acc: 0.756, val f1: 0.506\n",
      "Step: 120/360, val acc: 0.667, val f1: 0.502\n",
      "Step: 125/360, val acc: 0.700, val f1: 0.476\n",
      "Step: 130/360, val acc: 0.733, val f1: 0.484\n",
      "Step: 135/360, val acc: 0.733, val f1: 0.491\n",
      "Step: 140/360, val acc: 0.678, val f1: 0.461\n",
      "Step: 145/360, val acc: 0.733, val f1: 0.547\n",
      "Step: 150/360, val acc: 0.711, val f1: 0.503\n",
      "Step: 155/360, val acc: 0.711, val f1: 0.484\n",
      "Step: 160/360, val acc: 0.678, val f1: 0.506\n",
      "Step: 165/360, val acc: 0.733, val f1: 0.535\n",
      "Step: 170/360, val acc: 0.711, val f1: 0.503\n",
      "Step: 175/360, val acc: 0.700, val f1: 0.515\n",
      "Step: 180/360, val acc: 0.689, val f1: 0.463\n",
      "Step: 185/360, val acc: 0.711, val f1: 0.422\n",
      "Step: 190/360, val acc: 0.700, val f1: 0.476\n",
      "Step: 195/360, val acc: 0.689, val f1: 0.455\n",
      "Step: 200/360, val acc: 0.700, val f1: 0.488\n",
      "Step: 205/360, val acc: 0.644, val f1: 0.467\n",
      "Step: 210/360, val acc: 0.689, val f1: 0.506\n",
      "Step: 215/360, val acc: 0.767, val f1: 0.551\n",
      "Step: 220/360, val acc: 0.744, val f1: 0.520\n",
      "Step: 225/360, val acc: 0.744, val f1: 0.514\n",
      "Step: 230/360, val acc: 0.733, val f1: 0.494\n",
      "Step: 235/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 240/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 245/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 250/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 255/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 260/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 265/360, val acc: 0.711, val f1: 0.482\n",
      "Step: 270/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 275/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 280/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 285/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 290/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 295/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 300/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 305/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 310/360, val acc: 0.722, val f1: 0.487\n",
      "Step: 315/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 320/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 325/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 330/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 335/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 340/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 345/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 350/360, val acc: 0.733, val f1: 0.507\n",
      "Step: 355/360, val acc: 0.733, val f1: 0.507\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "model = ClinicalBERT(5)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "warmup_steps = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "timestamp = datetime.datetime.now()\n",
    "date = timestamp.strftime(\"%Y%m%d\")\n",
    "time = timestamp.strftime(\"%H%M%S\")\n",
    "save_path = f'./results/{date}/{time}'\n",
    "total_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "lr_scheduler = None if warmup_steps is None \\\n",
    "                    else torch.optim.lr_scheduler.LambdaLR(optimizer, partial(lr_lambda, warmup_steps=warmup_steps, \n",
    "                                                                                         total_steps=total_steps))\n",
    "train_loss_list = []\n",
    "writer = SummaryWriter(log_dir=os.path.join(save_path, 'tb_logs'))\n",
    "model.to(device)\n",
    "for step, epoch, batch in zip(range(total_steps), epoch_iter(NUM_EPOCHS, train_dataloader), itertools.cycle(train_dataloader)):\n",
    "    batch = {key: value.to(device) for key, value in batch.items()}\n",
    "    loss_val = train_step(\n",
    "        optimizer=optimizer,\n",
    "        model=model,\n",
    "        batch=batch,\n",
    "    )\n",
    "    writer.add_scalar(\"learning_rate\", optimizer.param_groups[0]['lr'], step)\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    writer.add_scalar(\"epoch\", epoch, step)\n",
    "    writer.add_scalar(\"loss/train\", loss_val, step)\n",
    "    train_loss_list.append(loss_val)\n",
    "    if step % 5 == 0 and step != 0:\n",
    "        val_results = eval_loop(\n",
    "            model=model,\n",
    "            dataloader=val_dataloader,\n",
    "            device=device\n",
    "        )\n",
    "        for key, value in val_results.items():\n",
    "            writer.add_scalar(f\"{key}/val\", value, step)\n",
    "        print(\"Step: {}/{}, val acc: {:.3f}, val f1: {:.3f}\".format(\n",
    "            step, \n",
    "            total_steps,\n",
    "            val_results[\"accuracy\"],\n",
    "            val_results[\"f1_score\"])\n",
    "        )\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee3b5efc781680bf777f6c0f6ded34256a070c94fe240b983fb244a284c14fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('ds_1012': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
